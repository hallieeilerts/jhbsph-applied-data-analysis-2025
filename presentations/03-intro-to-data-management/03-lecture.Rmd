---
title: "3 - Introduction to data management and cleaning"
author: | 
  | Hallie Eilerts-Spinelli
  | Johns Hopkins Bloomberg School of Public Health
  | Applied Data Analysis
  | \texttt{\href{mailto:heilert1@jh.edu}{heilert1@jh.edu}}
date: "March 2025"
output: 
   beamer_presentation:
    keep_tex: true
    latex_engine: xelatex
    highlight: espresso
    toc: false
header-includes:
  - \input{myheader.tex}
colorlinks: true
urlcolor: JHBlue
linkcolor: HeritageBlue
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.align="center")
options(tinytex.verbose = TRUE)
library(knitr)
library(kableExtra)
library(tidyr)
```

# Introduction

## Motivation (I)

Things (data, code, resources, drafts, etc.) can become disorganized as any research project progresses.

```{r, echo = FALSE}
include_graphics("figures/messy-files.jpg")
```

If you're not careful, this disorganization may seep into your analysis, compromising your results.

## Motivation (II)

**Following a structured approach to setting up your project is one of the best things you can do to ensure validity, efficiency, clarity, and reproducibility.**


## Agenda

1.  Directory structure
2.  Workflow
3.  Introduction to data management



# Directory structure

## Directory structure (I)

\scriptsize

Before you start working with data, you should decide how you will structure and name files and folders. Ensuring that data, code, and results are systematically arranged makes it easier to conduct analysis, debug, and collaborate.

\textcolor{JHOrange}{Basic project structure organized by file type.}

\tiny

```
project
 |
 |-- data
 |  |  
 |  +-- SurveyRaw2024.xlsx
 |  +-- README.txt 
 |    
 |-- code
 |  |  
 |  +-- data-prep.R
 |  +-- data-analysis.R 
 |    
 |-- results
 |  |  
 |  +-- figure1.jpeg
 |  +-- figure2.jpeg
 |  +-- table1.doc
 |    
```

## Directory structure (II)

\scriptsize

\textcolor{JHOrange}{More elaborated structure. Subfolders for data and code and folders for documentation and different types of outputs.}

\tiny

```
project
 |
 |-- data
 |  |  
 |  |-- raw-data
 |  |-- processed-data
 |    
 |-- code
 |  |  
 |  |-- cleaning
 |  |-- analysis 
 |    
 |-- documentation
 |  |  
 |  |-- ethics
 |  |   +-- ethics-approval.pdf 
 |  |   +-- consent-form.docx
 |  |-- methods
 |  |   +-- methods_decription.docx 
 |  |   +-- README.txt
 |    
 |-- outputs
 |  |  
 |  |-- figures
 |  |-- tables
 |  |-- paa-conference-2025
 |  |   +-- manuscript01_20241002.docx
 |  |   +-- references.bib
 |    
```

## Directory structure (III)

\scriptsize

\textcolor{JHOrange}{Pipeline style, separating source code from generated outputs.}

\tiny

```
project
 |  
 +-- make.R
 |-- data
 |-- src
 |  |-- data-preparation
 |      +-- prepare_session.R
 |      +-- clean_data.R
 |  |-- analysis
 |      +-- calc_summary_stats.R
 |      +-- fit_model.R
 |  |-- figures
 |      +-- visualizations.R
 |-- gen
 |  |  
 |  |-- data-preparation
 |      |-- output
 |          +-- survey-clean-20240206.csv
 |      |-- temp
 |          +-- duplicates.csv
 |  |-- analysis
 |      |-- audit
 |          +-- correlation-plots.pdf
 |      |-- output
 |          +-- model-fit.RData
 |  |-- figures
 |      |-- output
 |          +-- coverage-trend_2000-2020.pdf
```

## Folder hierarchy

-   Does it work better for your project to have a \textcolor{JHOrange}{flat} structure with many files in a few folders?
    -   Or a \textcolor{JHOrange}{nested} structure with multiple subfolders and a few files in each?
-   Avoid too many files in each folder (\>10 or 15)
-   Avoid excessive levels of folders (\>3 or 4)
-   Document your project/folder structure in a `README` file

## Data

-   Back up raw data securely
-   If you have multiple datasets, create a separate subfolder for each
-   Include codebook or `README` describing data. Include...
    -   From whom/where
    -   When it was received/procured
    
## Code and outputs

-   Keep scripts separate from outputs they generate
-   Do not store copies of the same file in different folders

## Naming folders and files

-   Use standard ASCII alphanumeric characters
-   Avoid spaces, dots
-   Clear but short names
-   Useful elements in a filename
    -   Indication of the content (abbreviated or encoded)
    -   Date (in the format \textcolor{JHOrange}{YYYYMMDD})
    -   Version number
-   Use hyphens or dashes to separate different elements in your filename
-   **Do not** use ambiguous descriptions of the version, such as `_new`, `_lastversion` or `_revised`

\tiny

\makebox[\textwidth]{(\href{https://libguides.uvt.nl/rdmstudent/filenames}{Tilburg University})}


# Workflow (a bit of theory)

## Data analysis pipeline

building a data analysis pipeline

![](figures/data-prep-theory.png){width="350px"}

\tiny

<https://dprep.hannesdatta.com/docs/modules/week2/>

## Steps in data preparation

Data preparation theory: components of source code

```         
1) Initializing the script/setup. Setup such as loading packages, setting variables, making database connection
    Loading packages, Making connections to databases, Any other “main” parameters (e.g., “Knowing” whether to prototype or not)
2) Input (e.g., loading data)
  Read data (e.g., unstructured/unstructured data, remote/local locations, files or databases)
3) Transformation
    e.g., filtering, aggregation, merging, transformation, deduplication (more later) 
    many options…: filtering, grouping and summarizing, creating new variables, etc.
    mostly leads to a “different primary key than the input data” (–> unit of analysis!)
4) Output (e.g., saving, passing on to the next script)
   Store (intermediate) data, figures (e.g., auditing, final)
```

Each and every source code file obeys to the “setup-ITO” (input, transformation, output) procedure.

<https://dprep.hannesdatta.com/docs/modules/week2/tutorial/tutorial#/4>

## Flow

Data should flow through a script

load data, analyze, save output

## Modularization

Split code into chunks. Everything with an input and output.

![](figures/modularization.png){width="350px"}

\tiny

<https://dprep.hannesdatta.com/docs/modules/week5/>

## Make file

Include a \textcolor{JHAccentRed}{\tt make} file sourcing scripts in the order necessary to execute the analysis

## Guidelines

-   Include a `makefile` with scripts run in order necessary to execute analysis
    -   Alternatively, include a `readme` with project description and instructions for how to run/build the project
-   Ensure file names are relative and not absolute

# Data exploration

## Somalia R2HC dataset

\tiny

Data description: This file is a long version of the data from all 3 time points. 

## Loading data

```{r}
# clear environment
rm(list = ls())
```

```{r, echo = FALSE}
yourFilePath <- "C:/Users/HEilerts/Institute of International Programs Dropbox/Hallie Eilerts-Spinelli/Teaching/AppliedDataAnalysis"
```

```{r}
# remember to install packages at first time use
# install.packages("haven")

# call package at the start of new session
library(haven)

# load data
dat <- read_dta(paste0(yourFilePath, 
                       "/jhbsph-applied-data-analysis-2025/data/R2HC_MainPaperData.dta"))

# view data in separate window
# View(data)

# number of columns
ncol(dat)

# number of rows
nrow(dat)

# column names
names(dat)[1:10]
```

## Data types

\tiny

```{r}
# check data structure
class(dat)

# check type of data stored in columns
sapply(dat[,1:5], class)

# look closer at labelled values
unique(dat$time_datacollect)

# assign factor labels as values
dat <- haven::as_factor(dat, levels="labels")
unique(dat$time_datacollect)
```

## Dates

\tiny

If there are dates in your dataset, make sure they are in date format.

```{r}
df <- data.frame(obs_n = 1:3,
                 date = c("2003/08/21", "2004/07/10", "2010/06/05"))
class(df$date)

# convert column to date format
df$date <- as.Date(df$date)

# format date
df$formatted_date <- format(df$date, "%m/%d/%Y")
class(df$formatted_date)

df
```

## Checking for missing values

\tiny

```{r}
# returns TRUE if there are any NAs
any(is.na(dat))  

# returns TRUE if there are any blank spaces
any(dat == "", na.rm = TRUE)

# returns TRUE if there are any blank spaces of any size
any(trimws(dat) == "", na.rm = TRUE)  

# identify columns with blank spaces
cols_with_blanks <- colnames(dat)[colSums(dat == "", na.rm = TRUE) > 0]

# look at columns with blank spaces
# View(dat[,cols_with_blanks])

# replace blank spaces with NA
dat[dat == ""] <- NA

# check again if any blank spaces
any(dat == "", na.rm = TRUE)

# number of NAs in each column
colSums(is.na(dat))[c(1,5,6,7)]  # Shows the count of NAs in each column

# number of rows with no NAs
nrow(dat[complete.cases(dat), ])
```

## Unit of analysis (I)

\tiny

```{r}
# find columns with "id" in their name
names(dat)[grepl("id", names(dat), ignore.case = TRUE)]

# number of rows in data frame
nrow(dat)

# number of unique values in id variables
length(unique(dat$hhid))
length(unique(dat$id_chld))
length(unique(dat$id_child))

# number of times each child id is repeated
library(tidyr)
library(dplyr)
dat %>%
  count(id_child, name = "obs_per_child") %>%
  count(obs_per_child, name = "n")
```

## Unit of analysis (II)

\tiny

```{r}
# check data collection times
unique(dat$time_datacollect)

# check if id_child is a unique identifier within each time point
nrow(subset(dat, time_datacollect == "Baseline"))
length(unique(subset(dat, time_datacollect == "Baseline")$id_child))

nrow(subset(dat, time_datacollect == "Midline")) == 
  length(unique(subset(dat, time_datacollect == "Midline")$id_child))

nrow(subset(dat, time_datacollect == "Endline")) ==
  length(unique(subset(dat, time_datacollect == "Endline")$id_child))

# create a variable to uniquely identify rows
dat <- dat[order(dat$hhid, dat$id_child, dat$time_datacollect),]
col_order <- c("recnr", names(dat))
dat$recnr <- 1:nrow(dat)
dat <- dat[,col_order]

head(dat[,1:5])
```

## Operationalizing variables

-   Standardize naming conventions to ensure uniqueness, improve clarity
-   Find a compromise between length and clarity

## Data exploration checklist

*  How many rows are there in the data?
*  How many columns?
*  What are the columns? 
*  Do values in these columns seem complete? 
*  Do values all make sense?
*  Are values properly encoded? 
*  What are common columns across (multiple) files (potential for merging!)?
*  What uniquely defines a row?
*  What is the unit of analysis (the specific entity or level of observation at which you analyze data)
   * Why is it important? It determines how you aggregate, summarize, and interpret your data. Without knowing your unit of analysis – at each part of your project – you will likely get lost.

<https://dprep.hannesdatta.com/docs/modules/week4/tutorial/tutorial#/6>

<https://dprep.hannesdatta.com/docs/modules/week2/tutorial/tutorial#/25>



