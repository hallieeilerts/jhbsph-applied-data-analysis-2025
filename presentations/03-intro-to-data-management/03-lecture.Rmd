---
title: "3 - Introduction to data management and cleaning"
author: | 
  | Hallie Eilerts-Spinelli
  | \texttt{\href{mailto:heilert1@jh.edu}{heilert1@jh.edu}}
  | Johns Hopkins Bloomberg School of Public Health
  | Applied Data Analysis
date: "February 2024"
output: 
   beamer_presentation:
    keep_tex: true
    latex_engine: xelatex
    highlight: espresso
    toc: true
header-includes:
  - \input{myheader.tex}
colorlinks: true
urlcolor: JHBlue
linkcolor: HeritageBlue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(tinytex.verbose = TRUE)
library(knitr)
library(kableExtra)
library(tidyr)
```

# Introduction

## Recap

-   General steps for answering a research question
      - Research question -\> statistical variables
-   Exploring variables related to research question (e.g. exposure, outcome, effect modifier, confounder)
-   Data analysis methods to answer different research questions
-   Appropriate data analysis methods for individual research question/datasets and draft analysis plan

## Agenda

1.  Presentation of concepts and associated R and Stata code
2.  Merging
3.  De-duplication
4.  Re-shaping
5.  Missing values
6.  Imputation methods
7.  Creating a directory

## Objectives

-   Use GitHub (or similar tools) for managing empirical research projects (e.g., Issues and Project Boards)
-   Use Git/GitHub (or similar tools) for versioning files and collaborating on privately-shared and publicly-available (open science) code repositories
-   Use R to clean and transform data for analysis (e.g., aggregation, merging, de-duplication, reshaping, data conversions, regular expressions)
-   Use R for generating automatic reports (e.g., to assess data quality, to report research findings in a paper) and deploying research findings in novel ways (e.g., apps)
-   Use Workflow Management Tools to create and run portable, automated, and reproducible data pipelines

# R bootcamp

## R introduction

-   Why R?
    -   Free, open-source
        -   the software source code is available and may be redistributed and modified
    -   Provides broad spectrum of statistical and graphical techniques with a wide range of applications
        -   This presentation was made in R!
    -   Large community of R users
    -   Reproducibility
-   R vs. RStudio
    -   R is a statistical programming language
    -   RStudio is an editor, user-friendly graphical interface
-   Drawbacks
    -   Learning curve
    -   Many ways to do the same thing

## Instructions for downloading R and RStudio

Download R from <https://www.r-project.org/>

Download RStudio from <https://posit.co/downloads/>

More instructions: <https://datacarpentry.github.io/r-socialsci/>

## Basics

The simplest usage of R is as a calculator. You can get output from R simply by typing math in the console:

```{r}
3 + 5
```

R can be used to store the results of calculations and many other kinds of objects.

```{r}
foo <- 2
foo
```

You can then use those objects in other operations.

```{r}
foo*8
```

## Scripts

The commands you type in the console will be forgotten when you close the session.

For your code and workflow to be reproducible, type commands in the script editor and save it.

RStudio allows you to execute commands directly from the script editor by using the \myhl{Ctrl} + \myhl{Enter} shortcut (on Mac, \myhl{Cmd} + \myhl{Return}).

## Functions (I)

Most of the work in R is done through functions. There are many functions already implemented in base R (i.e., we do not need to load a package to use them).

Function \textcolor{JHAccentRed}{\tt c()} concatenates values into a vector.

```{r}
myVec <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
myVec
```

Function \textcolor{JHAccentRed}{\tt sum()} returns the sum of all present in its arguments.

```{r}
sum(myVec)
```

## Functions (II)

\footnotesize

How do you know how use a function? How do you know what arguments it will accept?

-   Search the function in the **Help** pane
-   Type `help("[function name]")` in the console
-   Type `?[function name]` in the console
-   Type `example("[function name]")` in the console
-   Google it!

If a function is not working...

```{r}
problematicVec <- c(7, NA, 4, 2, NA)
sum(problematicVec)
```

...check the documentation and troubleshoot.

```{r}
sum(problematicVec, na.rm = TRUE)
```

## Functions (III)

\footnotesize

We can also define our own bespoke functions. Here is a simple one:

```{r}
addValues <- function(x,y){x+y}
addValues(3,4)
```

**If the same operations need to be repeated throughout your project, it is much better to use functions than copying and pasting your code.**

Save these in a file called \textcolor{JHAccentRed}{\tt util} or \textcolor{JHAccentRed}{\tt functions} that gets sourced at the top of any script they are used in.

\centering
![](figures/loading-functions.jpg){width="250px"}

## Functions (IV)

\tiny

Example of a complicated function that will make your life easier if it needs to be called multiple times.

```{r}
fitGLMmodel <- function(df, covariate_metadata) {

  training_data <- df %>%
    ungroup() %>%
    mutate(
      year1 = year - 1980,
      year2 = year1*year1
    ) %>%
    select(any_of(c("value_raw", "year", "year1", "year2", "gdp", "sex", "country")))
  
  # GLM regression
  fit <- glm(formula = covariate_metadata$glm_formula,
             data = training_data,
             family = covariate_metadata$glm_family,
             na.action = na.omit)

  # Save information about the fit
  capture.output(anova(fit), file=str_glue("./gen/{covariate_metadata$name}/audit/glm_anova.txt"))
  capture.output(confint(fit), file=str_glue("./gen/{covariate_metadata$name}/audit/glm_confint.txt"))
  capture.output(summary(fit), file=str_glue("./gen/{covariate_metadata$name}/audit/glm_summary.txt"))
  
  # Append fitted data to input frame
  value_glm = as_tibble(predict(fit, newdata=training_data, type="response")) %>% rename(value_glm=value)
  result <- cbind(df, value_glm)
  
  # Plot fit
  ggplot(result %>% filter(!is.na(value_raw)), aes(x = value_raw, y = value_glm)) + geom_point()
  ggsave(str_glue("./gen/{covariate_metadata$name}/audit/glm_fit.png"), width = 14, height = 14)
  
  return(result)
}
```

## Packages

R packages are extensions to the programming language. They contain code, data, and documentation.

Packages can be downloaded from the internet by typing in the console:

```{r, eval = FALSE}
install.packages("package_name")
```

Once a package is installed on your computer, you do not need to install it again. However, you need to load it every time you start a new R session.

```{r, eval = FALSE}
library("package_name")
```


## Popular packages

-   Loading data
    -   `xlsx`: allows for reading xls/xlsx spreadsheets into R
    -   `haven`: allows for reading SAS, SPSS, and Stata files into R
-   Data manipulation
    -   `tidyr`: gather, spread, putting data in [tidy format](file:///C:/Users/HEilerts/Downloads/v59i10.pdf)
    -   `dplyr`: subsetting, summarizing, joining
    -   `data.table`: data management, useful for big data
    -   `purrr`: tools for working with functions and vectors
    -   `lubridate`: tools that make it easier to work with dates
    -   `stringr`: tools for parsing strings
-   Modeling
    -   `car`: tools for regression analysis
    -   `mgcv`: generalized additive models
    -   `survival`: tools for survival analysis
-   Visualization and reporting results
    -   `ggplot2`: customizable graphics and plots
    -   `knitr`, `flextable`, `xtable`: generate output tables for html, Word, pdfs, \LaTeX


## `Base` vs `tidyverse` vs `data.table` (I)

\small

These packages represent three different paradigms in R programming used primarily for data manipulation, analysis, and visualization.

Choosing between them means deciding which toolset you want to use for your data tasks, based on factors like ease of use, performance, and personal or project requirements.

\centering

\scriptsize

```{r, echo = FALSE, results="asis"}

# Create the data frame for the table
comparison_table <- data.frame(
  Feature = c(
    "Ease of Use", "Performance", "Readability", "Flexibility", "Memory Usage",
    "Dependencies", "Community", "Learning Curve", "Strengths", "Weaknesses"
  ),
  Base_R = c(
    "Familiar to R users but can be verbose.",
    "Adequate for small datasets.",
    "Can be less readable for complex tasks.",
    "Built-in functions for most tasks.",
    "Moderate, can struggle with larger datasets.",
    "No external packages required.",
    "Core R support with extensive resources.",
    "Moderate, procedural style.",
    "Versatile, minimal dependencies.",
    "Verbose, less efficient for big data."
  ),
  Tidyverse = c(
    "Intuitive and user-friendly syntax.",
    "Decent for small, medium datasets.",
    "Highly readable and beginner-friendly.",
    "Excellent for tidy data workflows.",
    "Higher due to temporary objects.",
    "Requires multiple packages (e.g., `dplyr`).",
    "Large and active.",
    "Low, declarative and pipeline-based.",
    "Simplifies common workflows, consistent.",
    "Slower with large datasets, higher memory."
  ),
  data_table = c(
    "Concise but steep learning curve.",
    "Extremely fast for large datasets.",
    "Compact but may sacrifice readability.",
    "Great for advanced joins and aggregations.",
    "Optimized for low memory usage.",
    "Requires one package (`data.table`).",
    "Smaller but growing.",
    "High, unique syntax.",
    "High performance, large data handling.",
    "Less intuitive for beginners."
  )
)
# Render the table with kable
knitr::kable(comparison_table, format = "latex", col.names = c("Feature", "Base R", "Tidyverse", "data.table")) %>%
  kableExtra::row_spec(0, bold = TRUE) %>%  # Bold the first row (header)
  kableExtra::kable_styling(latex_options = "scale_down")
```

## `Base` vs `tidyverse` vs `data.table` (II)

\centering

![](figures/base-tidy-dt-functions.jpg){width="350px"} \tiny <https://mgimond.github.io/rug_2019_12/Index.html>

## Loops

Loops allow you to repeat a block of code multiple times.

\centering

![](figures/for-vs-while-loop.jpg){width="350px"}

## When to loop? (I)

\scriptsize

1. sequential dependencies

    -   \scriptsize when the current iteration depends on the result of the previous
 
\tiny
\vspace{1em}
  
```{r, eval = FALSE}
result[1] <- 3
for(i in 2:10){
  result[i] <- result[i - 1] + i
}
```

\scriptsize

2. modifying external objects, writing files

\tiny
\vspace{1em}

```{r, eval = FALSE}
for (i in 1:5) {
    plot(1:10, (1:10)^i, main = paste("Exponent:", i))
    ggsave(paste0("plot_", i, ".png"))
}
```

\scriptsize

3. interfacing with external systems

    -   \scriptsize querying a database, making API calls that don't support parallel or batch processing

## When to loop? (II)

\scriptsize

4. complex logic or conditions

    -   \scriptsize if/else conditions or nested logic for when to execute the action

5. debugging

    -   \scriptsize running the same operation on lots of data and identifying that element that trips up your code

6. performance doesn't matter

    -   \scriptsize simple operation prioritizing readability over speed

## Vectorized operations

\scriptsize

A programming technique that performs multiple operations at once instead of using loops to repeat operations one at a time.

\vspace{1em}

```{r}
x <- c(4, 9, 16)
sqrt(x)
```

\vspace{1em}

```{r}
x <- c(1, 2, 3)
y <- c(4, 5, 6)
x + y
```

Alternatively, as a loop...

```{r, eval = FALSE}
x <- c(1, 2, 3)
y <- c(4, 5, 6)
z <- c()
for(i in 1:length(x)){
  z[i] <- x[i] + y[i]
}
```

## The "apply family" (R) (I)

\scriptsize

A family of functions in base R that perform operations on elements like vectors, matrices, data frames, and lists in a more concise way than writing loops.

`apply` operates over rows or columns of a matrix, returns a vector

\tiny

```{r}
# construct a 5 x 6 matrix
my_matrix <- matrix(1:30, nrow = 5, ncol = 6)

# calculate the sum of each column
apply(my_matrix, 2, sum)
```

\scriptsize

`sapply` operates over a vector or list, returns a vector

\tiny

```{r}
# example list of character vectors
char_list <- list(
  animals = c("cat", "dog", "elephant"),
  fruits = c("apple", "banana"),
  colors = c("red", "blue", "green", "yellow")
)

# calculate the length of each element in the list
sapply(char_list, length)
```


## `lapply`

\scriptsize

`lapply` operates over a vector or list and returns a list. It is extremely useful for working with multiple surveys with a standard format.

\vspace{1em}

\tiny

```{r, eval = FALSE}
# define function to clean individual recode data from DHS surveys
fn_cleanIR <- function(x){
  
  x$weight <- x$v005/1000000
  
  if("v024" %in% names(x)){
    x$region_name <- haven::as_factor(x$v024)
  }else{
    x$region_name <- haven::as_factor(x$v101)
  }
  
  # trim white space
  x$myvcal1 <- as.character(x$vcal_1)
  # recode strings with non-alphanumeric characters as NA
  x$myvcal1 <- ifelse(grepl("[^a-zA-Z0-9[:space:]]", x$myvcal1), NA, x$myvcal1)
  
  return(x)
}

# clean datasets stored in list
l_ir_clean <- lapply(l_ir, fn_cleanIR)
```


## RStudio projects

-   Keep all the files associated with a given project together
    -   Input data, scripts, analytical results, and figures
-   *File* \> *New Project*
    -   *File* \> *New File* \> *R Script*
-   When an R project is opened within RStudio...
    -   A new R session is started
    -   The current working directory is set to the project directory
    -   Previously edited documents are restored
    -   RStudio settings are restored
-   Version control
    -   Projects can be connected to a version control repository on Github

## R Markdown

\footnotesize

R Markdown is a flexible type of document that integrates R code with Markdown (like this presentation). 

It allows you to seamlessly combine executable R code and its output with text, and can be readily converted to formats like PDF, Word, and HTML. Widely used for statistical analysis, reporting, and sharing reproducible research.

The `rmarkdown` package comes pre-installed with RStudio.

-   *File* \> *New File* \> *R Markdown*

[R Markdown cheat sheet](https://rstudio.github.io/cheatsheets/html/rmarkdown.html)


# Setting up a research project

## Getting organized

Things can become disorganized and difficult to navigate as any project progresses...

\vspace{1em}

**Following a structured approach to setting up your project is one of the best things you can do to ensure validity, efficiency, clarity, and reproducibility.**

## Directory structure

\small

Ensuring that data, code, and results are systematically arranged makes it easier to conduct analysis, debug, and create outputs.

General guidelines:

-   Back up raw data securely
-   If you have multiple datasets, create a separate subfolder for each
    -   Include codebook or README describing data
-   Keep scripts separate from outputs they generate
-   Think about folder hierarchy
    -   Does it work better for your project to have a flat structure with many files in a few folders; or multiple subfolders with a few files in each?
    -   Avoid too many files in each folder (>10 or 15)
    -   Avoid excessive levels of folders (>3 or 4)
-   Document your project/folder structure in a README
-   Do not store copies of the same file in different folders

## Example directory structure (I)

\tiny

```       
Project
 |
 |-- data
 |  |  
 |  |-- raw-data
 |  |-- processed-data
 |    
 |-- analysis
 |  |  
 |  |-- scripts
 |  |   +-- data-prep.R
 |  |   +-- data-analysis.R
 |  |-- visualizations
 |    
 |-- documentation
 |  |  
 |  |-- ethics
 |  |   +-- ethics-approval.pdf 
 |  |   +-- consent-form.docx
 |  |-- methods
 |  |   +-- methods_decription.docx 
 |  |   +-- README.txt
 |    
 |-- reports
 |  |  
 |  |-- figures
 |  |-- tables
 |  |-- paa-conference-2025
 |  |   +-- manuscript01_20241002.docx
 |  |   +-- references.bib
 |    
```



## Example directory structure (II)

\tiny

```       
Project
 |-- data
 |  |  
 |  +-- SurveyRaw2023.xlsx
 |  +-- README.txt 
 |-- src
 |  |-- make.R  
 |  |-- data-preparation
 |      +-- prepare_session.R
 |      +-- clean_data.R
 |  |-- analysis
 |      +-- calc_summary_stats.R
 |      +-- fit_model.R
 |  |-- figures
 |      +-- visualizations.R
 |-- gen
 |  |  
 |  |-- data-preparation
 |      |-- output
 |          +-- survey-clean-20240206.csv
 |      |-- temp
 |          +-- duplicates.csv
 |  |-- analysis
 |      |-- audit
 |          +-- correlation-plots.pdf
 |      |-- output
 |          +-- model-fit.RData
 |  |-- figures
 |      |-- output
 |          +-- plot-results.pdf
```

\begin{textblock*}{5cm}(7cm,2cm)
\begin{beamercolorbox}[rounded=true,shadow=true]{title}

\footnotesize

-   Create subdirectory for source code, with each step of pipeline getting its own subdirectory: \textcolor{JHAccentRed}{\tt /src/[pipeline-stage-name]/}

\vspace{1em}

-   Establish corresponding subdirectories within \textcolor{JHAccentRed}{\tt /gen/[pipeline-stage-name]/} with sub-subdirectories for generated files: \textcolor{JHAccentRed}{\tt temp}, \textcolor{JHAccentRed}{\tt output}, \textcolor{JHAccentRed}{\tt audit}

\vspace{1em}

-   Include a \textcolor{JHAccentRed}{\tt make} file sourcing scripts in the order necessary to execute the analysis

\end{beamercolorbox}
\end{textblock*}

\tiny 

\centering

[Tilburg Science Hub](https://tilburgsciencehub.com/topics/automation/workflows/auditing/workflow-checklist/)

## Creating folders

\small

Create directories and subdirectories manually, or using code:

```{r, eval = FALSE}
dir.create("./data")
dir.create("./src")
dir.create("./src/data-preparation")
dir.create("./gen")
dir.create("./gen/data-preparation")
dir.create("./gen/data-preparation/output")

# only create if does not exist
if(!dir.exists("./data")){dir.create("./data")}
```

## Naming files

-  Use standard ASCII alphanumeric characters
-  Clear but short filenames
-  Useful elements in a filename can be indication of the content (abbreviated or encoded), date (in the format \textcolor{JHAccentRed}{YYYYMMDD}), version number
-  Avoid spaces, dots
-  Use hyphens or dashes to separate different elements in your filename
-  **Do not** use ambiguous descriptions of the version, such as *\_new*, *\_lastversion* or *\_revised*

\centering
\tiny
[Tilburg University](https://libguides.uvt.nl/rdmstudent/filenames)


# Coding style

## Coding style

\small

If your code is messy, it's hard to be confident in your results. Messy code also be difficult to understand, debug, and tweak (especially when you come back to it a few weeks/months later!).

\vspace{1em}

**Using a consistent coding style throughout your project is important for clarity and quality control.**

\vspace{1em}

Find a style guide and stick to it. Examples:

-   [Advanced-R Styleguide](http://adv-r.had.co.nz/Style.html)
-   [Google's R Styleguide](https://google.github.io/styleguide/Rguide.html)

## Notation and naming (I)

\footnotesize

Adopt a case style to improve readability and be consistent in usage. Some options:

1. Variable names as descriptive nouns in `camelCase` or `snake_case`

```{r, eval = FALSE}
# camel
outcomesTable
summaryStats
# snake
model_output
data_long
```

2. Functions as descriptive verbs in `PascalCase`

```{r, eval = FALSE}
MakeBoxPlot()
FetchValue()
```

Note: R allows "." in variable names and functions but this goes against best practices in other coding languages. 

## Notation and naming (II)

\footnotesize

3. Save outputs in `kebab-case`

    -   \footnotesize Use a suffix can to indicate alterations of parent object (*survey-sample_females.csv*).

4. Usage of variable type as a prefix. Can improve clarity, but also more verbose.


```{r, eval = FALSE}
df_results   # data frame of results
l_results    # list of results

df_countries # data frame with country info
v_countries  # vector of country names

dat          # original data
datLong      # data reshaped long
datWide      # data rehaped wide

fn_makeBoxPlot() # custom function for making boxplot
```

## Syntax

-   Use two spaces per level of indentation
-   Put spaces around operators (e.g., +, -, *), after commas
-   An opening curly brace is never on it's own line, but a closing curly brace always is
-   Use `<-` when assigning values to variables in R
    -   Reserve `=` for argument passing in functions
    -   Note the important difference between `=` and `==` in R
-   Useful [RStudio addin](https://www.r-bloggers.com/2016/10/align-assign-rstudio-addin-to-align-assignment-operators/) that helps align code neatly
    
## Organization

\footnotesize

-   Load packages at the top of script
-   Comment your code!
-   Organize code into blocks to accomplish a single goal, describe that goal in comment
-   Use in-line comments sparingly
-   Adopt a file header that is included in every script and allows it to be interpreted on its own

\vspace{1em}

\tiny 

```{r, eval = FALSE}
################################################################################
# @project Example Project
# @description This file is responsible for cleaning column names
# @return Data frame with cleaned column names
################################################################################
```


\centering

[kmishra9 best practices](https://github.com/kmishra9/Best-Practices-for-Writing-R-Code)


# Workflow: building a data analysis pipeline

## Data preparation theory

![](figures/data-prep-theory.png){width="350px"}

\tiny
https://dprep.hannesdatta.com/docs/modules/week2/

## Steps in data preparation

Data preparation theory: components of source code

    1) Initializing the script/setup. Setup such as loading packages, setting variables, making database connection
        Loading packages, Making connections to databases, Any other “main” parameters (e.g., “Knowing” whether to prototype or not)
    2) Input (e.g., loading data)
      Read data (e.g., unstructured/unstructured data, remote/local locations, files or databases)
    3) Transformation
        e.g., filtering, aggregation, merging, transformation, deduplication (more later) 
        many options…: filtering, grouping and summarizing, creating new variables, etc.
        mostly leads to a “different primary key than the input data” (–> unit of analysis!)
    4) Output (e.g., saving, passing on to the next script)
       Store (intermediate) data, figures (e.g., auditing, final)

Each and every source code file obeys to the “setup-ITO” (input, transformation, output) procedure.

https://dprep.hannesdatta.com/docs/modules/week2/tutorial/tutorial#/4


## Flow

Data should flow through a script

load data
analyze
save output

## Modularization

Split code into chunks. Everything with an input and output.

![](figures/modularization.png){width="350px"}

\tiny
https://dprep.hannesdatta.com/docs/modules/week5/

## Guidelines

-  Include a `makefile` with scripts run in order necessary to execute analysis
   -  Alternatively, include a `readme` with project description and instructions for how to run/build the project
-   Ensure file names are relative and not absolute


# Data cleaning and exploration

## Example Data

\footnotesize

The SAFI (Studying African Farmer-Led Irrigation) Teaching Database, available on [FigShare](https://figshare.com/articles/SAFI_Survey_Results/6262019)

This is survey data relating to households and agriculture in Tanzania and Mozambique. The survey data was collected through interviews conducted between November 2016 and June 2017. This is a teaching version of the collected data; it is not the full dataset. The survey covered such things as; household features, agricultural practices, assets. and details about the household members.

\tiny 

Woodhouse, Philip; Veldwisch, Gert Jan; Brockington, Daniel; Komakech, Hans C.; Manjichi, Angela; Venot, Jean-Philippe (2018): SAFI Survey Results. <doi:10.6084/m9.figshare.6262019.v1>

## Common questions we can ask

What are columns in the data?
Do values in these columns seem complete? Do they all make sense? Are they properly encoded?
What are common columns across (multiple) files (potential for merging!)
What uniquely defines a row? 

https://dprep.hannesdatta.com/docs/modules/week4/tutorial/tutorial#/6

## Cleaning

-   Standardize naming conventions to ensure uniqueness, improve clarity
-   Find a compromise between length and clarity


## Unit of analysis

    Unit of analysis: Refers to the specific entity or level of observation at which you analyze data

    Why is it important? It determines how you aggregate, summarize, and interpret your data. Without knowing your unit of analysis – at each part of your project – you will likely get lost.

    Example: The episodes and rating data is organized along the individual episode-level. Using the column parentTconst in datasets[[1]], you could aggregate this data to another unit of analysis: the series level.

https://dprep.hannesdatta.com/docs/modules/week2/tutorial/tutorial#/25


## Verifying data types

    class(mobility$date)
    Can only calculate with dates when they are actual dates!

## Missing values

## Summary statistics

    We can generate summary statistics using the summary() command.
        do values make sense?
        are there any missing values?
    We can also zoom in on specific columns
        let's start making sense of the retail_and_recreation_percent_change_from_baseline variable: what does it mean?
        let's plot this variable over time!
    If our target unit of analysis is different from the unit of analysis in the current data set, we can also use aggregation.

## Plots

# Data wrangling

## Merging

## De-depulication

## Keys

general keys, primary, foreign

example with data table

## Imputation




## Hal to-do's for idea mining

Full tutorial of data prep: https://dprep.hannesdatta.com/docs/modules/week2/tutorial/intro-to-r
Full tutorial for engineering datasets: https://dprep.hannesdatta.com/docs/modules/week4/tutorial/data-preparation

## Resources

[R for Social Scientists](https://datacarpentry.github.io/r-socialsci/)

Data carpentry github: https://github.com/datacarpentry/r-socialsci/blob/main/episodes/00-intro.Rmd


[Introduction to R](https://www.datacamp.com/courses/free-introduction-to-r)

[Introduction to Github](https://github.com/skills/introduction-to-github)

## References

[RStudio User Guide](https://docs.posit.co/ide/user/)

[R for Social Scientistics](https://datacarpentry.github.io/r-socialsci/index.html)

[Tilburg Science Hub](https://tilburgsciencehub.com/)

[ClaytonJY R style guide](https://github.com/ClaytonJY/R-Styleguide?tab=readme-ov-file)

[Roman Pahl R bloggers style guide](https://www.r-bloggers.com/2019/11/my-r-style-guide/)
