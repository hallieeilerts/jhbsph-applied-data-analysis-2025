---
title: "3 - Project setup and data exploration"
author: | 
  | Hallie Eilerts-Spinelli
  | Johns Hopkins Bloomberg School of Public Health
  | Applied Data Analysis
  | \texttt{\href{mailto:heilert1@jh.edu}{heilert1@jh.edu}}
date: "March 2025"
output: 
   beamer_presentation:
    keep_tex: true
    latex_engine: xelatex
    highlight: espresso
    toc: false
header-includes:
  - \input{myheader.tex}
colorlinks: true
urlcolor: JHBlue
linkcolor: HeritageBlue
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.align="center")
options(tinytex.verbose = TRUE)
library(knitr)
library(kableExtra)
library(tidyr)
```

# Introduction

## Motivation (I)

Things (data, code, resources, drafts, etc.) can become disorganized as any research project progresses.

```{r, echo = FALSE}
include_graphics("figures/messy-files.jpg")
```

If you're not careful, this disorganization may seep into your analysis, compromising your results.

## Motivation (II)

**Following a structured approach to setting up your project is one of the best things you can do to ensure validity, efficiency, clarity, and reproducibility.**


## Agenda

1.  Setting up a project
2.  Research workflow
3.  Data exploration

# Setting up a project

## Directory structure (I)

\scriptsize

Before you start working with data, you should decide how you will structure files and folders. Ensuring that data, code, and results are systematically arranged makes it easier to conduct analysis, debug, and collaborate.

\textcolor{JHOrange}{Basic project structure organized by file type.}

\tiny

```
project
 |
 |-- data
 |  |  
 |  +-- SurveyRaw2024.xlsx
 |  +-- README.txt 
 |    
 |-- code
 |  |  
 |  +-- data-prep.R
 |  +-- data-analysis.R 
 |    
 |-- results
 |  |  
 |  +-- figure1.jpeg
 |  +-- figure2.jpeg
 |  +-- table1.doc
 |    
```

\begin{tikzpicture}[remember picture, overlay]
  \node[anchor=north east, draw, fill=white, text width=5cm] 
    at ([xshift = -2cm, yshift=-4cm] current page.north east) { % Adjust -2cm to move up
        \textbf{Tips} \\ 
        \begin{itemize}
          \item Back up raw data securely
          \item If you have multiple datasets, create a subfolder for each
          \item Include codebook or \textcolor{JHOrange}{README} describing when/from where data was received
          \item Keep scripts separate from outputs they generate
        \end{itemize}
    };
\end{tikzpicture}


## Directory structure (II)

\scriptsize

\textcolor{JHOrange}{More elaborated structure.} Subfolders for data and code, folders for documentation and different types of outputs.

\tiny

```
project
 |
 |-- data
 |  |  
 |  |-- survey-BD
 |  |-- world-bank
 |    
 |-- code
 |  |  
 |  |-- cleaning
 |  |-- analysis 
 |    
 |-- documentation
 |  |  
 |  |-- ethics
 |  |   +-- ethics-approval.pdf 
 |  |   +-- consent-form.docx
 |  |-- methods
 |  |   +-- methods_decription.docx 
 |  |   +-- README.txt
 |    
 |-- outputs
 |  |  
 |  |-- figures
 |  |-- tables
 |  |-- paa-conference-2025
 |  |   +-- manuscript01_20241002.docx
 |  |   +-- references.bib
 |    
```

## Naming folders and files

-   Clear but short names using standard ASCII alphanumeric characters
-   Avoid spaces, dots
-   Useful elements in a filename
    -   Indication of the content (abbreviated or encoded)
    -   Date (in the format \textcolor{JHOrange}{YYYYMMDD})
    -   Version number
-   Use hyphens or dashes to separate different elements
-   **Do not** use ambiguous descriptions of the version, such as `_new`, `_lastversion` or `_revised`

\tiny

\makebox[\textwidth]{(\href{https://libguides.uvt.nl/rdmstudent/filenames}{Tilburg University})}

## Coding style

\small

Messy code can be difficult to understand, debug, and update -- especially when you come back to it a few weeks/months later!.

\vspace{1em}

**Using a consistent coding style throughout your project can aid clarity and quality control.**

\vspace{1em}

Find a style guide and stick to it. Examples:

-   [Advanced-R Styleguide](http://adv-r.had.co.nz/Style.html)
-   [Google's R Styleguide](https://google.github.io/styleguide/Rguide.html)



## Script format

\footnotesize

-   Adopt a file header that is included in every script and allows it to be interpreted on its own
-   Load packages and data at the top of script
-   Organize code into blocks to accomplish a single goal, describe that goal in comment

\vspace{1em}

\tiny

```{r, eval = FALSE}
################################################################################
# @project Example Project
# @description This file is responsible for cleaning column names
# @return Data frame with cleaned column names
################################################################################
# clear environment
rm(list = ls())
# libraries
library(tidyverse)
# inputs
dat <- read.csv("./data/survey2024.csv")
################################################################################

# reshape data wide
dat %>%
  pivot_wider(names_from = variable, values_from = estimate)
```

\centering

\makebox[\textwidth]{(\href{https://github.com/kmishra9/Best-Practices-for-Writing-R-Code}{kmishra9})}

# Research workflow

## Project pipeline (I)

\scriptsize

Build a project pipeline that outlines the flow of tasks from loading data, cleaning, analysis, and report generation.

\centering

![](figures/research-pipeline-blank.png){width="350px"}

\tiny

\makebox[\textwidth]{(\href{https://dprep.hannesdatta.com/docs/modules/week5/}{Tilburg University})}


## Project pipeline (II)

\scriptsize

Data should flow through the project, with each script loading inputs, transforming the data, and creating a new output to pass along to the next script. 

\centering

![](figures/research-pipeline.png){width="350px"}
\tiny

\makebox[\textwidth]{(\href{https://dprep.hannesdatta.com/docs/modules/week5/}{Tilburg University})}

## Modularized code

\scriptsize
Breakdown analysis into code that performs small, manageable steps. If you make changes, you will only need to run the code from that step onwards.

\centering

![](figures/modularization.png){width="300px"}

\tiny

\makebox[\textwidth]{(\href{https://dprep.hannesdatta.com/docs/modules/week5/}{Tilburg University})}


## Align file directory structure with workflow

\scriptsize

\textcolor{JHOrange}{Data analysis pipeline structure with subfolders for pipeline stages.}

\tiny

```
project
 |  
 +-- make.R
 |-- data
 |-- src
 |  |-- data-preparation
 |      +-- prepare_session.R
 |      +-- clean_data.R
 |  |-- analysis
 |      +-- summary_stats.R
 |      +-- fit_model.R
 |-- gen
 |  |  
 |  |-- data-preparation
 |      |-- audit
 |          +-- duplicates.csv
 |      |-- output
 |          +-- survey-clean-20240206.csv
 |      |-- temp
 |          +-- unique-ids.csv
 |  |-- analysis
 |      |-- audit
 |          +-- correlation-plots.pdf
 |      |-- output
 |          +-- model-fit.RData
 |          +-- summary_table.csv
```

\begin{tikzpicture}[remember picture, overlay]
  \node[anchor=north east, draw, fill=white, text width=5cm] 
    at ([xshift = -2cm, yshift=-4cm] current page.north east) { % Adjust -2cm to move up
        \textbf{Tips} \\ 
        \begin{itemize}
          \item Include a \textcolor{JHOrange}{make} file sourcing scripts in the order necessary to execute the analysis
          \item Source code (\textcolor{JHOrange}{src}) organized in subfolders to match pipeline stage
          \item Each src subfolder has a corresponding subfolder for generated (\textcolor{JHOrange}{gen}) files
        \end{itemize}
    };
\end{tikzpicture}


# Data exploration

## Somalia R2HC dataset

Data description: [Insert description here] This file is a long version of the data from all 3 time points. 

## Load and view data

\tiny

```{r}
# clear environment
rm(list = ls())
```

```{r, echo = FALSE}
yourFilePath <- "C:/Users/HEilerts/Institute of International Programs Dropbox/Hallie Eilerts-Spinelli/Teaching/AppliedDataAnalysis"
```

```{r}
# remember to install packages at first time use
# install.packages("haven")

# call package at the start of new session
library(haven)

# load data
dat <- read_dta(paste0(yourFilePath, 
                       "/jhbsph-applied-data-analysis-2025/data/R2HC_MainPaperData.dta"))

# look at your data in a separate window
# View(data)

# print first five rows and first six columns
head(dat)[1:6]
```

## Explore data

\tiny

```{r}
# check number of columns
ncol(dat)
```
\vspace{10pt}

```{r}
# check number of rows
nrow(dat)
```
\vspace{10pt}

```{r}
# look at (subset of) column names
names(dat)[1:10]
```

## Check data types

\tiny

```{r}
# check data structure
class(dat)
```
\vspace{10pt}

```{r}
# check type of data stored in columns
sapply(dat[,1:5], class)
```

## Factors

\tiny

```{r}
# look closer at labelled values
unique(dat$time_datacollect)
```
\vspace{10pt}

```{r}
# can assign factor labels as values
dat <- haven::as_factor(dat, levels="labels")
unique(dat$time_datacollect)
```

## Dates

\tiny

If there are dates in your dataset, make sure they are in date format.

```{r}
# create dummy data
df <- data.frame(obs_n = 1:3,
                 date = c("2003/08/21", "2004/07/10", "2010/06/05"))
# check class of "date" column
class(df$date)
```
\vspace{10pt}

```{r}
# convert column to date format
df$date <- as.Date(df$date)

# format date and save as new column
df$formatted_date <- format(df$date, "%m/%d/%Y")
class(df$formatted_date)
```
\vspace{10pt}

```{r}
print(df)
```

## Character variables

\tiny

```{r}
head(dat$enum_baseline,10)
```
\vspace{10pt}

```{r}
# convert to lower case
dat$enum_baseline_clean <- tolower(dat$enum_baseline)
head(dat$enum_baseline_clean, 10)
```
\vspace{10pt}

```{r}
library(stringi)
# convert non-ASCII characters into their closest Latin character equivalent
dat$enum_baseline_clean <- stri_trans_general(dat$enum_baseline_clean, "Latin-ASCII")
```
\vspace{10pt}

```{r}
# create column for first name
dat$enum_firstname <- gsub("^(\\S+).*", "\\1", dat$enum_baseline_clean)
head(dat$enum_firstname, 10)
```

## Missing values

\tiny

```{r}
# returns TRUE if there are any NAs
any(is.na(dat))  
```
\vspace{10pt}

```{r}
# returns TRUE if there are any empty strings
any(dat == "", na.rm = TRUE)
```
\vspace{10pt}

```{r}
# returns TRUE if there are any blank spaces of any size
any(trimws(dat) == "", na.rm = TRUE)
```
\vspace{10pt}

```{r, eval = FALSE}
# identify columns with empty strings
cols_with_empty<- colnames(dat)[colSums(dat == "", na.rm = TRUE) > 0]

# look at columns with empty strings
View(dat[,cols_with_empty])
```

## Recode missing values

\tiny

```{r}
# replace empty strings with NA
dat[dat == ""] <- NA

# replace empty strings or blank spaces of any size with NA
dat[dat == "" | grepl("^\\s*$", dat)] <- NA

# check again if any empties
any(dat == "", na.rm = TRUE)
```
\vspace{10pt}


```{r}
# number of NAs in select columns
colSums(is.na(dat))[c(1,5,6,7)]
```
\vspace{10pt}

```{r}
# number of rows with no NAs
nrow(dat[complete.cases(dat), ])
```


## Check id variables

\tiny

```{r}
# find columns with "id" in their name
names(dat)[grepl("id", names(dat), ignore.case = TRUE)]
```
\vspace{10pt}

```{r}
# number of rows in data frame
nrow(dat)
```
\vspace{10pt}

```{r}
# number of unique values in id_child
length(unique(dat$id_child))
```

`id_child` does not uniquely identify the rows of the data frame.

## Observations per individual

\tiny

```{r}
# count the frequency of each id_child
head(table(dat$id_child))
```
\vspace{10pt}
```{r}
# add as column to data
dat$id_count <- table(dat$id_child)[dat$id_child]
```
\vspace{10pt}
```{r}
# do the same but with tidyverse
library(tidyverse)
dat %>%
  count(id_child, name = "obs_per_child") %>%
  count(obs_per_child, name = "n")
```
\vspace{10pt}

## Observation time

\tiny

```{r}
# data collection time variable
unique(dat$time_datacollect)
```
\vspace{10pt}

```{r}
# number of rows where time_datacollect is baseline
nrow(subset(dat, time_datacollect == "Baseline"))
```
\vspace{10pt}

```{r}
# number of unique child ids for basline
length(unique(subset(dat, time_datacollect == "Baseline")$id_child))
```
\vspace{10pt}

```{r}
nrow(subset(dat, time_datacollect == "Midline")) == 
  length(unique(subset(dat, time_datacollect == "Midline")$id_child))
```

When `time_datacollect` == "Baseline", there is one row per `id_child`.

## Create unique identifier

\tiny

```{r}
# sort data
dat <- dat[order(dat$hhid, dat$id_child, dat$time_datacollect),]
# vector of column names in desired order
col_order <- c("recnr", names(dat))
# create column recnr that is 1:N
dat$recnr <- 1:nrow(dat)
# arrange columns in data frame
dat <- dat[,col_order]

head(dat[,1:5])
```

## Save (somewhat) cleaned data

\tiny

```{r, eval = FALSE}
dat <- write.csv(dat, paste0(yourFilePath, 
                        "/jhbsph-applied-data-analysis-2025/gen/",
                        "r2hc-exploration-", 
                        format(Sys.Date(), "%Y%m%d"), ".csv"),
                 row.names = FALSE)
```


## Data exploration checklist

\scriptsize

*  How many rows are there in the data?
*  How many columns?
*  What are the columns? 
*  Do values in these columns seem complete? 
*  Do values all make sense?
*  Are values properly encoded? 
*  What are common columns across (multiple) files (potential for merging!)?
*  What uniquely defines a row?
*  What is the unit of analysis (the specific entity or level of observation at which you analyze data)
   * Why is it important? It determines how you aggregate, summarize, and interpret your data. Without knowing your unit of analysis – at each part of your project – you will likely get lost.

<https://dprep.hannesdatta.com/docs/modules/week4/tutorial/tutorial#/6>

<https://dprep.hannesdatta.com/docs/modules/week2/tutorial/tutorial#/25>



